{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "343a7eaa",
   "metadata": {},
   "source": [
    "# Handwritten Japanese Character Recognition (KMNIST Demo)\n",
    "\n",
    "This tool lets you **try out an AI model** that recognizes *handwritten Japanese characters* from the **KMNIST dataset**.  \n",
    "Don‚Äôt worry ‚Äî no programming knowledge or Japanese background is needed to use it.\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ What kind of AI is this?\n",
    "This demo uses **Artificial Intelligence (AI)** ‚Äî more specifically a branch called **Deep Learning**.\n",
    "\n",
    "- **AI** is the general idea: teaching computers to do tasks that usually need human intelligence.  \n",
    "- **Machine Learning** is a part of AI: instead of being programmed with rules, the computer *learns* from data.  \n",
    "- **Deep Learning** is a special kind of Machine Learning: it uses many ‚Äúlayers‚Äù of artificial **neural networks** (inspired by the brain) to find complex patterns in data.  \n",
    "\n",
    "Here we apply **Deep Learning for Image Recognition**:\n",
    "- The computer looks at your image (just 28√ó28 black-and-white dots).  \n",
    "- Through many layers, it learns to recognize strokes, curves, and shapes.  \n",
    "- Finally, it predicts which Japanese character your drawing most likely represents.\n",
    "\n",
    "---\n",
    "\n",
    "## üîé What is KMNIST?\n",
    "- KMNIST is a collection of **70,000 tiny images (28√ó28 pixels each)**.  \n",
    "- Each image shows **one Japanese Hiragana character** (basic sounds of Japanese).  \n",
    "- In this demo, the model is trained to recognize **10 different characters**:  \n",
    "  `o, ki, su, tsu, na, ha, ma, ya, re, wo`\n",
    "\n",
    "We also show you:\n",
    "- The character in **Hiragana script** (example: `„Åç`)  \n",
    "- The **English sound** (example: `\"kee\"`)  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úçÔ∏è How can you use this demo?\n",
    "You can try three ways:\n",
    "\n",
    "1. **Upload**  \n",
    "   - Upload any image.  \n",
    "   - The AI resizes it to 28√ó28 pixels and predicts what character it looks like.\n",
    "\n",
    "2. **Draw**  \n",
    "   - Use your mouse or trackpad to sketch inside the white box.  \n",
    "   - Click *Predict from drawing* to see what the AI thinks.\n",
    "\n",
    "3. **Sample**  \n",
    "   - Pick one of the 10 characters.  \n",
    "   - The app shows you a real dataset example and predicts.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä What do you see after prediction?\n",
    "- **Top-1 prediction**: The AI‚Äôs best guess, with probability and meaning.  \n",
    "- **Top-3 predictions**: Other close guesses.  \n",
    "- **Probability chart**: A bar graph of confidence levels.  \n",
    "- **Your 28√ó28 image**: Exactly what the model saw (click to zoom).  \n",
    "- **Reference grid**: Real dataset examples for comparison.  \n",
    "- **Sampled test image**: A random example (only in the *Sample* tab).\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why is this interesting?\n",
    "This demo is a small but powerful example of **Deep Learning in action**:\n",
    "- It shows how AI can **read handwriting**, even from tiny low-resolution images.  \n",
    "- The same technology (deep learning image recognition) powers many real-world tools:  \n",
    "  - Phone face unlock  \n",
    "  - Self-driving cars (recognizing roads, signs, pedestrians)  \n",
    "  - Medical imaging (detecting tumors in X-rays or MRI scans)  \n",
    "  - Translating handwritten text into digital form  \n",
    "\n",
    "Even if you don‚Äôt know Japanese, this demo is a fun way to see **how modern AI learns and makes predictions**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è A Note on Accuracy\n",
    "This demo shows how **AI (Deep Learning)** can recognize handwritten characters, but it‚Äôs not always perfect.  \n",
    "\n",
    "Why mistakes can happen:\n",
    "- **Training data**: The model has only seen a limited number of handwritten examples (28√ó28 pixel images). If your handwriting is very different, it may confuse the model.  \n",
    "- **Small size**: The images are tiny (just 28√ó28 dots), so fine details are lost.  \n",
    "- **Room for error**: Like humans, AI can misread unclear or unusual handwriting.  \n",
    "\n",
    "The important part:  \n",
    "- This is **normal in AI systems** ‚Äî even the best models have some error rate.  \n",
    "- With **more training data** and better models, the accuracy improves.  \n",
    "- Real-world AI (like in self-driving cars or medical imaging) is trained on **much larger and richer datasets**, which makes them much more reliable.  \n",
    "\n",
    "Think of this demo as a **fun, educational showcase**: it gives you a glimpse of how AI learns and predicts, but don‚Äôt expect it to always be 100% correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b48fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552da5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84e0390e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "* Running on local URL:  http://127.0.0.1:7894\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7894/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== KMNIST Inference + Auto-Discovered Models (native 28√ó28, click to zoom) ====\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "# ------------------------- Paths & discovery -------------------------\n",
    "def find_dir(dirname: str) -> Path:\n",
    "    \"\"\"\n",
    "    Find a directory named `dirname` in:\n",
    "      ./dirname, ../dirname, ../../dirname\n",
    "    Useful because this notebook lives under 'KMNIST/source code/'.\n",
    "    \"\"\"\n",
    "    here = Path.cwd()\n",
    "    for base in [here, here.parent, here.parent.parent]:\n",
    "        cand = base / dirname\n",
    "        if cand.exists() and cand.is_dir():\n",
    "            return cand\n",
    "    raise FileNotFoundError(f\"Could not find '{dirname}' near {here}\")\n",
    "\n",
    "DATA_DIR   = find_dir(\"data\")\n",
    "MODELS_DIR = find_dir(\"models\")\n",
    "\n",
    "def friendly_name_from_filename(p: Path) -> str:\n",
    "    \"\"\"Turn 'mlp_swa.pt' -> 'MLP Swa' (nice for UI).\"\"\"\n",
    "    return p.stem.replace(\"_\", \" \").title()\n",
    "\n",
    "def detect_arch_from_filename(p: Path) -> str:\n",
    "    \"\"\"\n",
    "    Return a key indicating which model architecture to build for this weight file.\n",
    "    Extend this function if you add other architectures (cnn, resnet, etc).\n",
    "    \"\"\"\n",
    "    name = p.stem.lower()\n",
    "    if any(k in name for k in [\"mlp\", \"perceptron\"]):\n",
    "        return \"mlp\"\n",
    "    # Example future extension:\n",
    "    # if \"cnn\" in name: return \"cnn\"\n",
    "    # if \"resnet\" in name: return \"resnet18\"\n",
    "    return \"mlp\"  # default\n",
    "\n",
    "def discover_models(models_dir: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Scan models_dir for *.pt files.\n",
    "    Returns: {ui_name: {\"path\": str, \"arch\": \"mlp\" | ...}}\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for f in sorted(models_dir.glob(\"*.pt\")):\n",
    "        out[friendly_name_from_filename(f)] = {\n",
    "            \"path\": str(f),\n",
    "            \"arch\": detect_arch_from_filename(f),\n",
    "        }\n",
    "    if not out:\n",
    "        raise FileNotFoundError(f\"No .pt files found in {models_dir}\")\n",
    "    return out\n",
    "\n",
    "ALL_MODELS = discover_models(MODELS_DIR)\n",
    "\n",
    "# ------------------------- Device -------------------------\n",
    "def pick_device():\n",
    "    if torch.backends.mps.is_available(): return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():         return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = pick_device()\n",
    "print(\"device:\", device)\n",
    "\n",
    "# ------------------------- Architectures -------------------------\n",
    "class MLP_Wide(nn.Module):\n",
    "    def __init__(self, p=0.35):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 512), nn.BatchNorm1d(512), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def build_model(arch_key: str) -> nn.Module:\n",
    "    \"\"\"Create a model instance for a given arch key.\"\"\"\n",
    "    if arch_key == \"mlp\":\n",
    "        return MLP_Wide(0.35)\n",
    "    # Example future extensions:\n",
    "    # if arch_key == \"cnn\": return YourCNN()\n",
    "    # if arch_key == \"resnet18\": return torchvision.models.resnet18(num_classes=10, ...)\n",
    "    raise ValueError(f\"Unknown architecture key: {arch_key}\")\n",
    "\n",
    "# We will (re)build & load per selection to allow different architectures in the future\n",
    "_current = {\"model\": None, \"arch\": None, \"name\": None}\n",
    "\n",
    "def load_selected_model(model_name: str) -> nn.Module:\n",
    "    \"\"\"Build (if needed) and load weights for the chosen model; return a ready model on `device`.\"\"\"\n",
    "    info = ALL_MODELS[model_name]\n",
    "    arch = info[\"arch\"]\n",
    "    path = info[\"path\"]\n",
    "\n",
    "    if _current[\"model\"] is None or _current[\"arch\"] != arch or _current[\"name\"] != model_name:\n",
    "        m = build_model(arch).to(device)\n",
    "        state = torch.load(path, map_location=device)\n",
    "        m.load_state_dict(state)\n",
    "        m.eval()\n",
    "        _current.update({\"model\": m, \"arch\": arch, \"name\": model_name})\n",
    "    return _current[\"model\"]\n",
    "\n",
    "# ------------------------- Labels -------------------------\n",
    "KMNIST_CLASSES = [\"o\",\"ki\",\"su\",\"tsu\",\"na\",\"ha\",\"ma\",\"ya\",\"re\",\"wo\"]\n",
    "KANA = {\"o\":\"„Åä\",\"ki\":\"„Åç\",\"su\":\"„Åô\",\"tsu\":\"„Å§\",\"na\":\"„Å™\",\"ha\":\"„ÅØ\",\"ma\":\"„Åæ\",\"ya\":\"„ÇÑ\",\"re\":\"„Çå\",\"wo\":\"„Çí\"}\n",
    "PRON = {\"o\":\"‚Äúoh‚Äù\",\"ki\":\"‚Äúkee‚Äù\",\"su\":\"‚Äúsoo‚Äù\",\"tsu\":\"‚Äútsoo‚Äù\",\"na\":\"‚Äúnah‚Äù\",\"ha\":\"‚Äúhah‚Äù\",\"ma\":\"‚Äúmah‚Äù\",\"ya\":\"‚Äúyah‚Äù\",\"re\":\"‚Äúreh‚Äù\",\"wo\":\"‚Äúoh/wo‚Äù\"}\n",
    "\n",
    "# ------------------------- Test set (for Sample tab) -------------------------\n",
    "_KX = _KY = None\n",
    "def load_kmnist_test():\n",
    "    global _KX, _KY\n",
    "    if _KX is not None:\n",
    "        return _KX, _KY\n",
    "    imgs = DATA_DIR / \"kmnist-test-imgs.npz\"\n",
    "    labs = DATA_DIR / \"kmnist-test-labels.npz\"\n",
    "    if not imgs.exists() or not labs.exists():\n",
    "        raise FileNotFoundError(f\"KMNIST test .npz not found in {DATA_DIR}\")\n",
    "    _KX = np.load(imgs)[\"arr_0\"]\n",
    "    _KY = np.load(labs)[\"arr_0\"]\n",
    "    return _KX, _KY\n",
    "\n",
    "# ------------------------- Helpers -------------------------\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "def preprocess_pil(pil: Image.Image):\n",
    "    \"\"\"PIL -> ([1,1,28,28] tensor on device, native 28√ó28 PIL).\"\"\"\n",
    "    pil = pil.convert(\"L\").resize((28, 28), Image.BILINEAR)\n",
    "    x = to_tensor(pil)[0]\n",
    "    if x.mean().item() > 0.5:  # invert if background light\n",
    "        x = 1.0 - x\n",
    "    native_28 = Image.fromarray((x.cpu().numpy() * 255).astype(np.uint8))\n",
    "    return x.unsqueeze(0).unsqueeze(0).to(device), native_28\n",
    "\n",
    "def probs_to_fig(probs, classes):\n",
    "    idx = np.argsort(-probs)[:10]\n",
    "    fig, ax = plt.subplots(figsize=(5, 2.8))\n",
    "    ax.bar(range(len(idx)), probs[idx])\n",
    "    ax.set_xticks(range(len(idx)))\n",
    "    ax.set_xticklabels([classes[i] for i in idx])\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel(\"prob.\")\n",
    "    ax.grid(axis=\"y\", alpha=0.25)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def small_ref_grid(label_idx: int, grid=2):\n",
    "    \"\"\"2√ó2 grid at native resolution (56√ó56).\"\"\"\n",
    "    try:\n",
    "        x_test, y_test = load_kmnist_test()\n",
    "    except Exception:\n",
    "        return Image.fromarray(np.zeros((56, 56), np.uint8))\n",
    "    idxs = np.where(y_test == label_idx)[0]\n",
    "    if len(idxs) == 0:\n",
    "        return Image.fromarray(np.zeros((56, 56), np.uint8))\n",
    "    rng = np.random.default_rng(12345 + label_idx)\n",
    "    picks = rng.choice(idxs, size=min(grid*grid, len(idxs)), replace=False)\n",
    "    canvas = Image.new(\"L\", (28*grid, 28*grid), 0)\n",
    "    r = c = 0\n",
    "    for idx in picks:\n",
    "        img = 255 - x_test[idx].astype(np.uint8)  # white-on-black like model input\n",
    "        canvas.paste(Image.fromarray(img), (c*28, r*28))\n",
    "        c += 1\n",
    "        if c == grid:\n",
    "            c = 0; r += 1\n",
    "            if r == grid: break\n",
    "    return canvas\n",
    "\n",
    "# ------------------------- Predictors -------------------------\n",
    "@torch.no_grad()\n",
    "def predict_from_pil(pil_img: Image.Image, which_model: str):\n",
    "    if pil_img is None:\n",
    "        return \"‚Äî\", {}, None, None, None\n",
    "    model = load_selected_model(which_model)\n",
    "    x, native_28 = preprocess_pil(pil_img)\n",
    "    probs = F.softmax(model(x), dim=1)[0].cpu().numpy()\n",
    "    top1 = int(np.argmax(probs))\n",
    "    romaji = KMNIST_CLASSES[top1]\n",
    "    md = f\"**Top-1:** `{romaji}` {KANA[romaji]} *(p={probs[top1]:.3f}; {PRON[romaji]})*\"\n",
    "    top3 = {KMNIST_CLASSES[i]: float(probs[i]) for i in np.argsort(-probs)[:3]}\n",
    "    fig  = probs_to_fig(probs, KMNIST_CLASSES)\n",
    "    ref  = small_ref_grid(top1, grid=2)\n",
    "    return md, top3, fig, native_28, ref\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_from_sketch(data, which_model: str):\n",
    "    if data is None:\n",
    "        return \"‚Äî\", {}, None, None, None\n",
    "\n",
    "    # Accept multiple payload variants from gr.Sketchpad\n",
    "    arr = None\n",
    "    if isinstance(data, dict):\n",
    "        if data.get(\"image\") is not None:\n",
    "            arr = data[\"image\"]\n",
    "        elif data.get(\"composite\") is not None:\n",
    "            arr = data[\"composite\"]\n",
    "        elif data.get(\"layers\") is not None:\n",
    "            imgs = []\n",
    "            for layer in data[\"layers\"]:\n",
    "                li = layer.get(\"image\") if isinstance(layer, dict) else layer\n",
    "                if li is not None:\n",
    "                    imgs.append(np.asarray(li))\n",
    "            if imgs:\n",
    "                arr = np.maximum.reduce(imgs)\n",
    "    else:\n",
    "        arr = np.asarray(data)\n",
    "\n",
    "    if arr is None:\n",
    "        return \"‚Äî\", {}, None, None, None\n",
    "\n",
    "    arr = np.asarray(arr)\n",
    "    if arr.ndim == 3:\n",
    "        arr = arr.mean(axis=2)\n",
    "    arr = arr.astype(np.float32)\n",
    "    if arr.max() <= 1.0:\n",
    "        arr = (arr * 255).astype(np.uint8)\n",
    "    else:\n",
    "        arr = arr.astype(np.uint8)\n",
    "\n",
    "    pil = Image.fromarray(arr).convert(\"L\")\n",
    "    return predict_from_pil(pil, which_model)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_from_label(label_str: str, which_model: str):\n",
    "    try:\n",
    "        x_test, y_test = load_kmnist_test()\n",
    "    except Exception as e:\n",
    "        return f\"‚Äî (test set not found: {e})\", {}, None, None, None, None\n",
    "    if label_str not in KMNIST_CLASSES:\n",
    "        return \"‚Äî\", {}, None, None, None, None\n",
    "\n",
    "    label_idx = KMNIST_CLASSES.index(label_str)\n",
    "    idxs = np.where(y_test == label_idx)[0]\n",
    "    if len(idxs) == 0:\n",
    "        return \"‚Äî\", {}, None, None, None, None\n",
    "\n",
    "    pil28 = Image.fromarray(x_test[np.random.choice(idxs)]).convert(\"L\")\n",
    "    pil28_inv = Image.fromarray(255 - np.array(pil28))  # native 28√ó28, white-on-black\n",
    "    md, top3, fig, native_28, ref = predict_from_pil(pil28, which_model)\n",
    "    return md, top3, fig, native_28, ref, pil28_inv\n",
    "\n",
    "# ------------------------- UI -------------------------\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## KMNIST ‚Äî Handwritten Kana („Å≤„Çâ„Åå„Å™) Recognizer\")\n",
    "    gr.Markdown(\n",
    "        \"Upload, draw, or sample a character. Images are shown at **true 28√ó28**; \"\n",
    "        \"use the ‚§¢ button on any image to zoom. Choose a model below ‚Äî \"\n",
    "        \"the list is auto-discovered from your `models/` folder.\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            model_choice = gr.Dropdown(\n",
    "                choices=list(ALL_MODELS.keys()),\n",
    "                value=list(ALL_MODELS.keys())[0],\n",
    "                label=\"Select Model\",\n",
    "                multiselect=False   # ensures single select\n",
    "            )\n",
    "\n",
    "            with gr.Tab(\"Upload\"):\n",
    "                up = gr.Image(type=\"pil\", image_mode=\"L\", label=\"Upload\")\n",
    "                btn_up = gr.Button(\"Predict from upload\")\n",
    "            with gr.Tab(\"Draw\"):\n",
    "                pad = gr.Sketchpad(label=\"Draw\")\n",
    "                btn_pad = gr.Button(\"Predict from drawing\")\n",
    "            with gr.Tab(\"Sample\"):\n",
    "                sample_label = gr.Dropdown(choices=KMNIST_CLASSES, value=\"ki\", label=\"Pick class\")\n",
    "                btn_sample = gr.Button(\"Sample & predict\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            top1_out  = gr.Markdown(\"Top-1: ‚Äî\")\n",
    "            top3_out  = gr.Label(num_top_classes=3, label=\"Top-3\")\n",
    "            chart_out = gr.Plot(label=\"Probabilities\")\n",
    "            your28_out = gr.Image(type=\"pil\", label=\"Your 28√ó28 (native)\",\n",
    "                                  show_fullscreen_button=True, interactive=False)\n",
    "            ref_out    = gr.Image(type=\"pil\", label=\"Reference (2√ó2, 56√ó56 native)\",\n",
    "                                  show_fullscreen_button=True, interactive=False)\n",
    "            sample_out = gr.Image(type=\"pil\", label=\"Sampled test image (28√ó28 native)\",\n",
    "                                  show_fullscreen_button=True, interactive=False)\n",
    "\n",
    "    btn_up.click(predict_from_pil,     inputs=[up,  model_choice],\n",
    "                 outputs=[top1_out, top3_out, chart_out, your28_out, ref_out])\n",
    "    btn_pad.click(predict_from_sketch, inputs=[pad, model_choice],\n",
    "                 outputs=[top1_out, top3_out, chart_out, your28_out, ref_out])\n",
    "    btn_sample.click(predict_from_label, inputs=[sample_label, model_choice],\n",
    "                 outputs=[top1_out, top3_out, chart_out, your28_out, ref_out, sample_out])\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e799e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
