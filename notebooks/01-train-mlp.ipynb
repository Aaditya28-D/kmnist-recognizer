{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dd0fc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using repo root: /Users/aaditya/Workspace/Learning/Github/projects/kmnist-recognizer/notebooks\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Detect project root (the folder containing src/, data/, models/, etc.)\n",
    "REPO_ROOT = Path(__file__).resolve().parent.parent if \"__file__\" in globals() else Path.cwd()\n",
    "\n",
    "# Ensure `src/` is importable\n",
    "if str(REPO_ROOT / \"src\") not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT / \"src\"))\n",
    "\n",
    "print(\"Using repo root:\", REPO_ROOT)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "MODELS_DIR = REPO_ROOT / \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc80a36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# project imports\n",
    "from src.kmnist.models.mlp import MLPWide\n",
    "from src.kmnist.train import train_model, pick_device\n",
    "from src.kmnist.labels import KMNIST_CLASSES\n",
    "\n",
    "DATA_DIR   = Path(\"/Users/aaditya/Workspace/Learning/Github/projects/kmnist-recognizer/data\")\n",
    "MODELS_DIR = Path(\"models\")\n",
    "\n",
    "device = pick_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a44b1b0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/aaditya/Workspace/Learning/Github/projects/kmnist-recognizer/notebooks/data/kmnist-train-imgs.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#DATA_DIR = Path(\"/Users/aaditya/Workspace/Learning/Github/projects/kmnist-recognizer/data\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m x_train = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkmnist-train-imgs.npz\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33marr_0\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      3\u001b[39m y_train = np.load(DATA_DIR / \u001b[33m\"\u001b[39m\u001b[33mkmnist-train-labels.npz\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[33m\"\u001b[39m\u001b[33marr_0\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      5\u001b[39m x_test  = np.load(DATA_DIR / \u001b[33m\"\u001b[39m\u001b[33mkmnist-test-imgs.npz\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[33m\"\u001b[39m\u001b[33marr_0\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/lib/npyio.py:427\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    425\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     fid = stack.enter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    428\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/aaditya/Workspace/Learning/Github/projects/kmnist-recognizer/notebooks/data/kmnist-train-imgs.npz'"
     ]
    }
   ],
   "source": [
    "#DATA_DIR = Path(\"/Users/aaditya/Workspace/Learning/Github/projects/kmnist-recognizer/data\")\n",
    "x_train = np.load(DATA_DIR / \"kmnist-train-imgs.npz\")[\"arr_0\"]\n",
    "y_train = np.load(DATA_DIR / \"kmnist-train-labels.npz\")[\"arr_0\"]\n",
    "\n",
    "x_test  = np.load(DATA_DIR / \"kmnist-test-imgs.npz\")[\"arr_0\"]\n",
    "y_test  = np.load(DATA_DIR / \"kmnist-test-labels.npz\")[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "020d9a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class KMNISTArrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps numpy arrays (H,W) uint8 -> torch tensors [1,28,28] float in [0,1].\n",
    "    No inversion is applied (KMNIST is already dark bg / light fg).\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = torch.from_numpy(self.x[i]).float() / 255.0     # [28,28] -> [0,1]\n",
    "        img = img.unsqueeze(0)                                # [1,28,28]\n",
    "        label = int(self.y[i])\n",
    "        return img, label\n",
    "\n",
    "full_train_ds = KMNISTArrayDataset(x_train, y_train)\n",
    "test_ds       = KMNISTArrayDataset(x_test,  y_test)\n",
    "len(full_train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73d1b3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 5000, 10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 55k train / 5k val split\n",
    "val_size = 5000\n",
    "train_size = len(full_train_ds) - val_size\n",
    "train_ds, val_ds = random_split(full_train_ds, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07ecf018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPWide(\n",
       "  (net): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): GELU(approximate='none')\n",
       "    (4): Dropout(p=0.35, inplace=False)\n",
       "    (5): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): GELU(approximate='none')\n",
       "    (8): Dropout(p=0.35, inplace=False)\n",
       "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): GELU(approximate='none')\n",
       "    (12): Dropout(p=0.35, inplace=False)\n",
       "    (13): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPWide(p=0.35)  # same architecture you used for the app\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4054060f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train acc 0.860 loss 0.457 | val acc 0.932 loss 0.225\n",
      "Epoch 02 | train acc 0.928 loss 0.235 | val acc 0.948 loss 0.174\n",
      "Epoch 03 | train acc 0.944 loss 0.183 | val acc 0.950 loss 0.159\n",
      "Epoch 04 | train acc 0.954 loss 0.151 | val acc 0.955 loss 0.142\n",
      "Epoch 05 | train acc 0.963 loss 0.119 | val acc 0.957 loss 0.140\n",
      "Epoch 06 | train acc 0.966 loss 0.107 | val acc 0.961 loss 0.131\n",
      "Epoch 07 | train acc 0.970 loss 0.095 | val acc 0.965 loss 0.127\n",
      "Epoch 08 | train acc 0.974 loss 0.083 | val acc 0.964 loss 0.128\n",
      "Epoch 09 | train acc 0.975 loss 0.077 | val acc 0.966 loss 0.124\n",
      "Epoch 10 | train acc 0.978 loss 0.068 | val acc 0.964 loss 0.127\n",
      "Epoch 11 | train acc 0.980 loss 0.059 | val acc 0.971 loss 0.115\n",
      "Epoch 12 | train acc 0.981 loss 0.057 | val acc 0.966 loss 0.136\n",
      "Epoch 13 | train acc 0.982 loss 0.053 | val acc 0.967 loss 0.120\n",
      "Epoch 14 | train acc 0.985 loss 0.049 | val acc 0.969 loss 0.122\n",
      "Epoch 15 | train acc 0.985 loss 0.047 | val acc 0.969 loss 0.123\n",
      "Epoch 16 | train acc 0.984 loss 0.046 | val acc 0.966 loss 0.128\n",
      "Epoch 17 | train acc 0.986 loss 0.044 | val acc 0.969 loss 0.123\n",
      "Epoch 18 | train acc 0.987 loss 0.039 | val acc 0.972 loss 0.115\n",
      "Epoch 19 | train acc 0.987 loss 0.041 | val acc 0.968 loss 0.129\n",
      "Epoch 20 | train acc 0.988 loss 0.036 | val acc 0.967 loss 0.131\n",
      "Epoch 21 | train acc 0.989 loss 0.035 | val acc 0.969 loss 0.130\n",
      "Epoch 22 | train acc 0.989 loss 0.033 | val acc 0.970 loss 0.133\n",
      "Epoch 23 | train acc 0.991 loss 0.029 | val acc 0.970 loss 0.136\n",
      "Epoch 24 | train acc 0.990 loss 0.029 | val acc 0.968 loss 0.147\n",
      "Epoch 25 | train acc 0.989 loss 0.032 | val acc 0.969 loss 0.134\n",
      "Epoch 26 | train acc 0.990 loss 0.030 | val acc 0.966 loss 0.140\n",
      "Early stopping at epoch 26 (no improvement for 8 epochs)\n",
      "[ckpt] Saved mlp_model_v002_acc0.972.pt (best so far remains 0.972)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.029718816976926545,\n",
       " 'train_acc': 0.9904181817228144,\n",
       " 'val_loss': 0.11457726734876633,\n",
       " 'model_path': 'models/mlp_model_v002_acc0.972.pt',\n",
       " 'best_val_acc': 0.9715999998092651}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "LR     = 3e-3\n",
    "\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    val_loader=val_loader,\n",
    "    out_path=MODELS_DIR,       # <â€” saved here\n",
    "    early_stopping=True,\n",
    "    patience=8,\n",
    "    min_delta=1e-4,\n",
    "    save_best_only=False      # final run is always saved as a new version; best.pt updated on improvement\n",
    ")\n",
    "\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c017ea79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/mlp_model_v001.pt', 'models/mlp_model_v002_acc0.972.pt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "sorted(glob(str(MODELS_DIR / \"mlp_model_v*.pt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e14d4555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, PosixPath('models/best.pt'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best model (updated whenever val loss improved)\n",
    "(Path(MODELS_DIR) / \"best.pt\").exists(), (MODELS_DIR / \"best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb0dbc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.387778296661377, 'acc': 0.9198}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    correct, total, loss_sum = 0, 0, 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss_sum += criterion(logits, yb).item() * xb.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        total   += xb.size(0)\n",
    "    return {\"loss\": loss_sum/total, \"acc\": correct/total}\n",
    "\n",
    "test_metrics = evaluate(model, test_loader, device)\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b27fa52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.318680917596817, 'acc': 0.9149}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = MLPWide(p=0.35)\n",
    "best_state = torch.load(MODELS_DIR / \"best.pt\", map_location=\"cpu\")\n",
    "best_model.load_state_dict(best_state)\n",
    "\n",
    "best_test_metrics = evaluate(best_model, test_loader, device)\n",
    "best_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b8727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Detect project root (the folder containing src/, data/, models/, etc.)\n",
    "REPO_ROOT = Path(__file__).resolve().parent.parent if \"__file__\" in globals() else Path.cwd()\n",
    "\n",
    "# Ensure `src/` is importable\n",
    "if str(REPO_ROOT / \"src\") not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT / \"src\"))\n",
    "\n",
    "print(\"Using repo root:\", REPO_ROOT)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "MODELS_DIR = REPO_ROOT / \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b17c992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e851fcdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
