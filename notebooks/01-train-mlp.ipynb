{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "218d102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c59afd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmnist-train-imgs.npz already exists\n",
      "kmnist-train-labels.npz already exists\n",
      "kmnist-test-imgs.npz already exists\n",
      "kmnist-test-labels.npz already exists\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import os.path\n",
    "\n",
    "def download_file(url):\n",
    "    path = url.split('/')[-1]\n",
    "    if os.path.isfile(path):\n",
    "        print (f\"{path} already exists\")\n",
    "    else:\n",
    "      r = requests.get(url, stream=True)\n",
    "      with open(path, 'wb') as f:\n",
    "          total_length = int(r.headers.get('content-length'))\n",
    "          print('Downloading {} - {:.1f} MB'.format(path, (total_length / 1024000)))\n",
    "          for chunk in tqdm(r.iter_content(chunk_size=1024), total=int(total_length / 1024) + 1, unit=\"KB\"):\n",
    "              if chunk:\n",
    "                  f.write(chunk)\n",
    "\n",
    "url_list = [\n",
    "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz',\n",
    "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz',\n",
    "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz',\n",
    "    'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz'\n",
    "]\n",
    "\n",
    "for url in url_list:\n",
    "    download_file(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "599e9db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network with PyTorch.ipynb kmnist-train-imgs.npz\n",
      "kmnist-test-imgs.npz              kmnist-train-labels.npz\n",
      "kmnist-test-labels.npz            perceptron.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5642227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b45dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc944b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "62f905c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import dataloader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "torch.manual_seed(28)\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57738338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 0.0468 sec\n",
      "MPS time: 0.0101 sec\n"
     ]
    }
   ],
   "source": [
    "### optional\n",
    "import torch, time\n",
    "\n",
    "# pick devices\n",
    "cpu = torch.device(\"cpu\")\n",
    "mps = torch.device(\"mps\")\n",
    "\n",
    "# matrix size (bigger = clearer speed difference)\n",
    "N = 4000  \n",
    "\n",
    "# create random tensor\n",
    "x_cpu = torch.randn(N, N, device=cpu)\n",
    "x_mps = torch.randn(N, N, device=mps)\n",
    "\n",
    "# warm-up MPS (first call compiles kernels)\n",
    "_ = x_mps @ x_mps\n",
    "\n",
    "# cpu timing\n",
    "start = time.time()\n",
    "y_cpu = x_cpu @ x_cpu\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "end = time.time()\n",
    "print(f\"CPU time: {end - start:.4f} sec\")\n",
    "\n",
    "# mps timing\n",
    "start = time.time()\n",
    "y_mps = x_mps @ x_mps\n",
    "torch.mps.synchronize()   # wait for GPU to finish\n",
    "end = time.time()\n",
    "print(f\"MPS time: {end - start:.4f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "b2dc9340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (60000, 28, 28) (60000,)\n",
      "test : (10000, 28, 28) (10000,)\n",
      "xb: torch.Size([128, 1, 28, 28]) torch.float32 0.0 1.0\n",
      "yb: torch.Size([128]) torch.int64 tensor([9, 4, 5, 1, 3, 9, 4, 7, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# load npz\n",
    "x_train = np.load(\"kmnist-train-imgs.npz\")[\"arr_0\"]   # (60000,28,28)\n",
    "y_train = np.load(\"kmnist-train-labels.npz\")[\"arr_0\"] # (60000,)\n",
    "x_test  = np.load(\"kmnist-test-imgs.npz\")[\"arr_0\"]    # (10000,28,28)\n",
    "y_test  = np.load(\"kmnist-test-labels.npz\")[\"arr_0\"]  # (10000,)\n",
    "\n",
    "print(\"train:\", x_train.shape, y_train.shape)\n",
    "print(\"test :\", x_test.shape,  y_test.shape)\n",
    "\n",
    "# convert to tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32).unsqueeze(1) / 255.0  # (60000,1,28,28)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)                          # (60000,)\n",
    "x_test_tensor  = torch.tensor(x_test,  dtype=torch.float32).unsqueeze(1) / 255.0\n",
    "y_test_tensor  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "# datasets + loaders\n",
    "BATCH_SIZE = 128\n",
    "train_ds = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_ds  = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# sanity check\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"xb:\", xb.shape, xb.dtype, xb.min().item(), xb.max().item())\n",
    "print(\"yb:\", yb.shape, yb.dtype, yb[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "02db9c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(28*28, 10)   # 784 → 10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)   # raw logits\n",
    "        return x\n",
    "\n",
    "model = LogisticRegression().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "943eeb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "9584fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d90587",
   "metadata": {},
   "source": [
    "### Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "242fd30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/5 | loss: 0.8380 | train acc: 0.7646\n",
      "epoch 2/5 | loss: 0.6518 | train acc: 0.8092\n",
      "epoch 3/5 | loss: 0.6219 | train acc: 0.8162\n",
      "epoch 4/5 | loss: 0.6064 | train acc: 0.8211\n",
      "epoch 5/5 | loss: 0.5964 | train acc: 0.8234\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "EPOCHS = 5  # start tiny to sanity-check, then try 20–30\n",
    "model.train()  # sets dropout/bn to training mode (good habit)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)          # images  [B, 1, 28, 28]\n",
    "        yb = yb.to(device)          # labels  [B]\n",
    "\n",
    "        # ----- forward\n",
    "        logits = model(xb)          # [B, 10] raw scores\n",
    "\n",
    "        # ----- loss\n",
    "        loss = criterion(logits, yb)  # CrossEntropy with class indices\n",
    "\n",
    "        # ----- backward\n",
    "        optimizer.zero_grad()       # clear old gradients\n",
    "        loss.backward()             # compute new gradients dL/dθ\n",
    "        optimizer.step()            # θ := θ - lr * grad\n",
    "\n",
    "        # track metrics\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)             # predicted class per sample\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    train_acc = correct / total\n",
    "    print(f\"epoch {epoch+1}/{EPOCHS} | loss: {avg_loss:.4f} | train acc: {train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "8c3480bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test | loss: 1.0078 | acc: 0.6988\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()    # eval mode: disables dropout, uses running stats in BN\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        logits = model(xb)                 # [B, 10]\n",
    "        loss = criterion(logits, yb)       # CE loss on this batch\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print(f\"test | loss: {test_loss:.4f} | acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba71fe",
   "metadata": {},
   "source": [
    "### MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "2507bd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),              # [B,1,28,28] -> [B,784]\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),           # regularization\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 10)         # logits (no Softmax)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MLP().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "2d8371b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)  # L2 helps generalization\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "9d9c3217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/15 | loss: 0.7287 | train acc: 0.7745\n",
      "epoch 2/15 | loss: 0.4006 | train acc: 0.8806\n",
      "epoch 3/15 | loss: 0.3319 | train acc: 0.9011\n",
      "epoch 4/15 | loss: 0.2932 | train acc: 0.9120\n",
      "epoch 5/15 | loss: 0.2711 | train acc: 0.9187\n",
      "epoch 6/15 | loss: 0.2516 | train acc: 0.9245\n",
      "epoch 7/15 | loss: 0.2360 | train acc: 0.9287\n",
      "epoch 8/15 | loss: 0.2272 | train acc: 0.9308\n",
      "epoch 9/15 | loss: 0.2197 | train acc: 0.9325\n",
      "epoch 10/15 | loss: 0.2121 | train acc: 0.9361\n",
      "epoch 11/15 | loss: 0.2041 | train acc: 0.9372\n",
      "epoch 12/15 | loss: 0.2004 | train acc: 0.9383\n",
      "epoch 13/15 | loss: 0.1930 | train acc: 0.9392\n",
      "epoch 14/15 | loss: 0.1919 | train acc: 0.9414\n",
      "epoch 15/15 | loss: 0.1874 | train acc: 0.9415\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    train_acc = correct / total\n",
    "    print(f\"epoch {epoch+1}/{EPOCHS} | loss: {avg_loss:.4f} | train acc: {train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "b5b557d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test | loss: 0.3912 | acc: 0.8898\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print(f\"test | loss: {test_loss:.4f} | acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "57df1d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/30 | train_acc 0.9456 | test_acc 0.8917\n",
      "epoch 2/30 | train_acc 0.9446 | test_acc 0.8915\n",
      "epoch 3/30 | train_acc 0.9438 | test_acc 0.8959\n",
      "epoch 4/30 | train_acc 0.9444 | test_acc 0.8963\n",
      "epoch 5/30 | train_acc 0.9464 | test_acc 0.8904\n",
      "epoch 6/30 | train_acc 0.9476 | test_acc 0.8956\n",
      "epoch 7/30 | train_acc 0.9460 | test_acc 0.8923\n",
      "epoch 8/30 | train_acc 0.9472 | test_acc 0.8948\n",
      "epoch 9/30 | train_acc 0.9493 | test_acc 0.8960\n",
      "Early stop at epoch 9; best test_acc = 0.8963\n"
     ]
    }
   ],
   "source": [
    "best_acc, wait, patience = 0.0, 0, 5\n",
    "best_state = None\n",
    "EPOCHS = 30  # allow more room; early stop will cut it short\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # ---- train (same as before)\n",
    "    model.train()\n",
    "    total_loss = 0.0; correct = 0; total = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "\n",
    "    train_loss = total_loss / total\n",
    "    train_acc  = correct / total\n",
    "\n",
    "    # ---- evaluate\n",
    "    test_loss, test_acc = evaluate(model, test_loader)\n",
    "    print(f\"epoch {epoch+1}/{EPOCHS} | train_acc {train_acc:.4f} | test_acc {test_acc:.4f}\")\n",
    "\n",
    "    # ---- early stop tracking\n",
    "    if test_acc > best_acc:\n",
    "        best_acc, wait = test_acc, 0\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stop at epoch {epoch+1}; best test_acc = {best_acc:.4f}\")\n",
    "            model.load_state_dict(best_state)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "41b3e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"max\", factor=0.1, patience=2\n",
    ")\n",
    "\n",
    "# after each test eval:\n",
    "scheduler.step(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "90663aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    ToTensor()\n",
       "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
       "    RandomAffine(degrees=[0.0, 0.0], translate=(0.1, 0.1))\n",
       ")"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, translate=(0.1,0.1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "0dc7f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP_BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(128, 10)  # logits\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "model = MLP_BN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# keep your ReduceLROnPlateau:\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.1, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "97181ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001\n"
     ]
    }
   ],
   "source": [
    "def current_lr(opt): return opt.param_groups[0]['lr']\n",
    "print(\"lr:\", current_lr(optimizer))\n",
    "# after scheduler.step(test_acc), print again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "f7d7ae0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28]) 0.0 1.0 torch.Size([128]) tensor([4, 9, 9, 3, 5, 9, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Augmentations: small rotations/shifts (works well for handwriting)\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.ToPILImage(),                 # HxW (uint8) -> PIL\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),                   # -> [1,H,W] float in [0,1]\n",
    "])\n",
    "test_tfms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class KMNISTNPZ(Dataset):\n",
    "    def __init__(self, x_np, y_np, tfm):\n",
    "        self.x = x_np\n",
    "        self.y = y_np\n",
    "        self.tfm = tfm\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.x[idx]                     # (28,28) uint8\n",
    "        lab = int(self.y[idx])                # scalar int\n",
    "        img = self.tfm(img)                   # -> [1,28,28] float\n",
    "        return img, torch.tensor(lab, dtype=torch.long)\n",
    "\n",
    "# rebuild loaders with augmentation for train only\n",
    "aug_train_ds = KMNISTNPZ(x_train, y_train, train_tfms)\n",
    "plain_test_ds = KMNISTNPZ(x_test,  y_test,  test_tfms)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(aug_train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(plain_test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# sanity check\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(xb.shape, xb.min().item(), xb.max().item(), yb.shape, yb[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "5752e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch, numpy as np\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),        # -> [1,28,28] in [0,1]\n",
    "])\n",
    "test_tfms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class KMNISTNPZ(Dataset):\n",
    "    def __init__(self, x_np, y_np, tfm):\n",
    "        self.x, self.y, self.tfm = x_np, y_np, tfm\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i):\n",
    "        img = self.tfm(self.x[i])                    # (28,28) -> [1,28,28]\n",
    "        lab = torch.tensor(int(self.y[i]), dtype=torch.long)\n",
    "        return img, lab\n",
    "\n",
    "train_loader = DataLoader(KMNISTNPZ(x_train, y_train, train_tfms),\n",
    "                          batch_size=128, shuffle=True)\n",
    "test_loader  = DataLoader(KMNISTNPZ(x_test,  y_test,  test_tfms),\n",
    "                          batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "04197577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001\n"
     ]
    }
   ],
   "source": [
    "def lr_now(opt): return opt.param_groups[0]['lr']\n",
    "print(\"lr:\", lr_now(optimizer))\n",
    "# after scheduler.step(test_acc): print(\"lr:\", lr_now(optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "cfe20409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28]) 0.0 1.0 tensor([6, 5, 0, 7, 2, 3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "print(xb.shape, xb.min().item(), xb.max().item(), yb[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "5e714d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1/40 | lr 0.00100 | train_loss 1.1385 acc 0.6280 | test_loss 0.8498 acc 0.7296\n",
      "epoch  2/40 | lr 0.00100 | train_loss 0.8431 acc 0.7271 | test_loss 0.7379 acc 0.7625\n",
      "epoch  3/40 | lr 0.00100 | train_loss 0.7560 acc 0.7593 | test_loss 0.6605 acc 0.7859\n",
      "epoch  4/40 | lr 0.00100 | train_loss 0.6980 acc 0.7759 | test_loss 0.6261 acc 0.7999\n",
      "epoch  5/40 | lr 0.00100 | train_loss 0.6629 acc 0.7896 | test_loss 0.5905 acc 0.8088\n",
      "epoch  6/40 | lr 0.00100 | train_loss 0.6370 acc 0.7967 | test_loss 0.5558 acc 0.8192\n",
      "epoch  7/40 | lr 0.00100 | train_loss 0.6187 acc 0.8036 | test_loss 0.5398 acc 0.8238\n",
      "epoch  8/40 | lr 0.00100 | train_loss 0.6039 acc 0.8103 | test_loss 0.5339 acc 0.8280\n",
      "epoch  9/40 | lr 0.00100 | train_loss 0.5873 acc 0.8139 | test_loss 0.5140 acc 0.8316\n",
      "epoch 10/40 | lr 0.00100 | train_loss 0.5748 acc 0.8178 | test_loss 0.5009 acc 0.8404\n",
      "epoch 11/40 | lr 0.00100 | train_loss 0.5594 acc 0.8224 | test_loss 0.4916 acc 0.8444\n",
      "epoch 12/40 | lr 0.00100 | train_loss 0.5673 acc 0.8202 | test_loss 0.5052 acc 0.8406\n",
      "epoch 13/40 | lr 0.00100 | train_loss 0.5531 acc 0.8268 | test_loss 0.4833 acc 0.8490\n",
      "epoch 14/40 | lr 0.00100 | train_loss 0.5553 acc 0.8262 | test_loss 0.4806 acc 0.8479\n",
      "epoch 15/40 | lr 0.00100 | train_loss 0.5404 acc 0.8310 | test_loss 0.4674 acc 0.8508\n",
      "epoch 16/40 | lr 0.00100 | train_loss 0.5334 acc 0.8320 | test_loss 0.4630 acc 0.8549\n",
      "epoch 17/40 | lr 0.00100 | train_loss 0.5284 acc 0.8341 | test_loss 0.4649 acc 0.8540\n",
      "epoch 18/40 | lr 0.00100 | train_loss 0.5302 acc 0.8323 | test_loss 0.4613 acc 0.8541\n",
      "epoch 19/40 | lr 0.00100 | train_loss 0.5265 acc 0.8337 | test_loss 0.4587 acc 0.8555\n",
      "epoch 20/40 | lr 0.00100 | train_loss 0.5228 acc 0.8350 | test_loss 0.4570 acc 0.8562\n",
      "epoch 21/40 | lr 0.00100 | train_loss 0.5233 acc 0.8346 | test_loss 0.4468 acc 0.8595\n",
      "epoch 22/40 | lr 0.00100 | train_loss 0.5177 acc 0.8383 | test_loss 0.4504 acc 0.8576\n",
      "epoch 23/40 | lr 0.00100 | train_loss 0.5148 acc 0.8383 | test_loss 0.4445 acc 0.8607\n",
      "epoch 24/40 | lr 0.00100 | train_loss 0.5171 acc 0.8379 | test_loss 0.4461 acc 0.8611\n",
      "epoch 25/40 | lr 0.00100 | train_loss 0.5101 acc 0.8375 | test_loss 0.4432 acc 0.8572\n",
      "epoch 26/40 | lr 0.00100 | train_loss 0.5149 acc 0.8396 | test_loss 0.4370 acc 0.8648\n",
      "epoch 27/40 | lr 0.00100 | train_loss 0.5070 acc 0.8409 | test_loss 0.4466 acc 0.8576\n",
      "epoch 28/40 | lr 0.00100 | train_loss 0.5087 acc 0.8422 | test_loss 0.4417 acc 0.8597\n",
      "epoch 29/40 | lr 0.00010 | train_loss 0.5012 acc 0.8429 | test_loss 0.4540 acc 0.8557\n",
      "epoch 30/40 | lr 0.00010 | train_loss 0.4955 acc 0.8451 | test_loss 0.4371 acc 0.8621\n",
      "epoch 31/40 | lr 0.00010 | train_loss 0.4872 acc 0.8463 | test_loss 0.4325 acc 0.8636\n",
      "Early stop at epoch 31. Best test_acc = 0.8648\n",
      "Restored best model | test_loss 0.4370 | test_acc 0.8648\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def current_lr(opt): \n",
    "    return opt.param_groups[0]['lr']\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "# ---- training with early stop + LR scheduler ----\n",
    "EPOCHS = 40\n",
    "patience = 5\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"max\", factor=0.1, patience=2  # drop LR 10× if no test_acc improvement for 2 epochs\n",
    ")\n",
    "\n",
    "best_acc = 0.0\n",
    "best_state = None\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Evaluate on test\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    # Scheduler step on the metric we want to maximize\n",
    "    scheduler.step(test_acc)\n",
    "\n",
    "    print(\n",
    "        f\"epoch {epoch:2d}/{EPOCHS} | \"\n",
    "        f\"lr {current_lr(optimizer):.5f} | \"\n",
    "        f\"train_loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
    "        f\"test_loss {test_loss:.4f} acc {test_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Early stopping bookkeeping\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        wait = 0\n",
    "        # keep a CPU copy to be device-agnostic\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stop at epoch {epoch}. Best test_acc = {best_acc:.4f}\")\n",
    "            break\n",
    "\n",
    "# Restore best weights (if improved at least once)\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    # optional: re-evaluate best model\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"Restored best model | test_loss {test_loss:.4f} | test_acc {test_acc:.4f}\")\n",
    "else:\n",
    "    print(\"No improvement recorded; best_state is None.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4ef802",
   "metadata": {},
   "source": [
    "### Lets make this model more deeper with slighet changes.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "cbd36317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP_Wide(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),                        # [B,1,28,28] -> [B,784]\n",
    "            nn.Linear(784, 512), nn.BatchNorm1d(512), nn.GELU(), nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.GELU(), nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.GELU(), nn.Dropout(0.4),\n",
    "            nn.Linear(128, 10)                   # logits\n",
    "        )\n",
    "    def forward(self, x): \n",
    "        return self.net(x)\n",
    "\n",
    "model = MLP_Wide().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "d2ac8c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion  = nn.CrossEntropyLoss(label_smoothing=0.05)  # helps generalization\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=3e-3, weight_decay=3e-4)\n",
    "\n",
    "# OneCycleLR needs steps_per_epoch\n",
    "steps_per_epoch = len(train_loader)\n",
    "EPOCHS = 30\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=3e-3,\n",
    "    epochs=EPOCHS, steps_per_epoch=steps_per_epoch,\n",
    "    pct_start=0.3,           # warmup portion\n",
    "    div_factor=10,           # initial lr = max_lr/div_factor\n",
    "    final_div_factor=100,    # final lr = initial/final_div_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "830be839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1/30 | lr 0.00038 | train_loss 1.4236 acc 0.5723 | test_loss 1.0453 acc 0.7201\n",
      "epoch  2/30 | lr 0.00062 | train_loss 1.0152 acc 0.7327 | test_loss 0.8702 acc 0.7832\n",
      "epoch  3/30 | lr 0.00098 | train_loss 0.9217 acc 0.7677 | test_loss 0.8472 acc 0.7899\n",
      "epoch  4/30 | lr 0.00142 | train_loss 0.8619 acc 0.7900 | test_loss 0.7603 acc 0.8281\n",
      "epoch  5/30 | lr 0.00188 | train_loss 0.8270 acc 0.8058 | test_loss 0.7474 acc 0.8328\n",
      "epoch  6/30 | lr 0.00233 | train_loss 0.7981 acc 0.8183 | test_loss 0.7516 acc 0.8331\n",
      "epoch  7/30 | lr 0.00268 | train_loss 0.7848 acc 0.8233 | test_loss 0.7587 acc 0.8300\n",
      "epoch  8/30 | lr 0.00292 | train_loss 0.7754 acc 0.8275 | test_loss 0.7118 acc 0.8451\n",
      "epoch  9/30 | lr 0.00300 | train_loss 0.7750 acc 0.8275 | test_loss 0.7336 acc 0.8317\n",
      "epoch 10/30 | lr 0.00298 | train_loss 0.7734 acc 0.8280 | test_loss 0.7447 acc 0.8281\n",
      "epoch 11/30 | lr 0.00293 | train_loss 0.7693 acc 0.8307 | test_loss 0.7084 acc 0.8456\n",
      "epoch 12/30 | lr 0.00285 | train_loss 0.7651 acc 0.8316 | test_loss 0.7212 acc 0.8435\n",
      "epoch 13/30 | lr 0.00274 | train_loss 0.7661 acc 0.8295 | test_loss 0.7432 acc 0.8277\n",
      "epoch 14/30 | lr 0.00260 | train_loss 0.7628 acc 0.8324 | test_loss 0.7042 acc 0.8457\n",
      "epoch 15/30 | lr 0.00244 | train_loss 0.7518 acc 0.8380 | test_loss 0.6861 acc 0.8577\n",
      "epoch 16/30 | lr 0.00225 | train_loss 0.7499 acc 0.8376 | test_loss 0.7275 acc 0.8411\n",
      "epoch 17/30 | lr 0.00205 | train_loss 0.7354 acc 0.8424 | test_loss 0.6741 acc 0.8676\n",
      "epoch 18/30 | lr 0.00183 | train_loss 0.7287 acc 0.8463 | test_loss 0.6754 acc 0.8635\n",
      "epoch 19/30 | lr 0.00161 | train_loss 0.7120 acc 0.8525 | test_loss 0.6610 acc 0.8678\n",
      "epoch 20/30 | lr 0.00139 | train_loss 0.7027 acc 0.8564 | test_loss 0.6337 acc 0.8793\n",
      "epoch 21/30 | lr 0.00117 | train_loss 0.6871 acc 0.8624 | test_loss 0.6423 acc 0.8742\n",
      "epoch 22/30 | lr 0.00095 | train_loss 0.6741 acc 0.8673 | test_loss 0.6215 acc 0.8847\n",
      "epoch 23/30 | lr 0.00075 | train_loss 0.6530 acc 0.8752 | test_loss 0.5993 acc 0.8905\n",
      "epoch 24/30 | lr 0.00057 | train_loss 0.6468 acc 0.8775 | test_loss 0.6101 acc 0.8882\n",
      "epoch 25/30 | lr 0.00040 | train_loss 0.6333 acc 0.8832 | test_loss 0.5878 acc 0.8961\n",
      "epoch 26/30 | lr 0.00026 | train_loss 0.6205 acc 0.8901 | test_loss 0.5782 acc 0.9017\n",
      "epoch 27/30 | lr 0.00015 | train_loss 0.6083 acc 0.8927 | test_loss 0.5789 acc 0.8996\n",
      "epoch 28/30 | lr 0.00007 | train_loss 0.5992 acc 0.8973 | test_loss 0.5768 acc 0.9014\n",
      "epoch 29/30 | lr 0.00002 | train_loss 0.5950 acc 0.8983 | test_loss 0.5732 acc 0.9022\n",
      "epoch 30/30 | lr 0.00000 | train_loss 0.5953 acc 0.8982 | test_loss 0.5691 acc 0.9050\n",
      "Restored best | test_loss 0.5691 | test_acc 0.9050\n"
     ]
    }
   ],
   "source": [
    "def lr_now(opt): return opt.param_groups[0]['lr']\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "best_acc, wait, patience = 0.0, 0, 15    # start best_acc low; patience 8 works well with OneCycle\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    run_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()               # <-- step scheduler EACH BATCH\n",
    "\n",
    "        run_loss += loss.item() * xb.size(0)\n",
    "        correct  += (logits.argmax(1) == yb).sum().item()\n",
    "        total    += xb.size(0)\n",
    "\n",
    "    train_loss = run_loss/total\n",
    "    train_acc  = correct/total\n",
    "\n",
    "    test_loss, test_acc = evaluate(model, test_loader)\n",
    "\n",
    "    print(f\"epoch {epoch:2d}/{EPOCHS} | lr {lr_now(optimizer):.5f} | \"\n",
    "          f\"train_loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
    "          f\"test_loss {test_loss:.4f} acc {test_acc:.4f}\")\n",
    "\n",
    "    # early stopping on test_acc\n",
    "    if test_acc > best_acc:\n",
    "        best_acc, wait = test_acc, 0\n",
    "        best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stop at epoch {epoch}; best test_acc = {best_acc:.4f}\")\n",
    "            break\n",
    "\n",
    "# restore best weights\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    test_loss, test_acc = evaluate(model, test_loader)\n",
    "    print(f\"Restored best | test_loss {test_loss:.4f} | test_acc {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bd07ae",
   "metadata": {},
   "source": [
    "the model below is slighlty weeker so we are using the previous model and doing three sets of parameter tunning on it below are three models as p1, p2, p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "e98a760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch: torch.Size([128, 1, 28, 28]) 0.0 1.0 tensor([1, 3, 7, 1, 3, 0, 2, 8])\n",
      "epoch  1/40 | lr 0.00035 | train_loss 0.8123 acc 0.8939 | test_loss 0.7757 acc 0.9029\n",
      "epoch  2/40 | lr 0.00048 | train_loss 0.8068 acc 0.8942 | test_loss 0.7749 acc 0.9043\n",
      "epoch  3/40 | lr 0.00070 | train_loss 0.8113 acc 0.8933 | test_loss 0.7821 acc 0.9001\n",
      "epoch  4/40 | lr 0.00098 | train_loss 0.8254 acc 0.8866 | test_loss 0.7943 acc 0.8940\n",
      "epoch  5/40 | lr 0.00130 | train_loss 0.8379 acc 0.8809 | test_loss 0.8100 acc 0.8826\n",
      "epoch  6/40 | lr 0.00165 | train_loss 0.8438 acc 0.8791 | test_loss 0.8012 acc 0.8917\n",
      "epoch  7/40 | lr 0.00200 | train_loss 0.8533 acc 0.8725 | test_loss 0.8108 acc 0.8822\n",
      "epoch  8/40 | lr 0.00233 | train_loss 0.8605 acc 0.8681 | test_loss 0.8165 acc 0.8804\n",
      "epoch  9/40 | lr 0.00260 | train_loss 0.8631 acc 0.8669 | test_loss 0.8261 acc 0.8773\n",
      "epoch 10/40 | lr 0.00282 | train_loss 0.8660 acc 0.8661 | test_loss 0.8290 acc 0.8766\n",
      "epoch 11/40 | lr 0.00295 | train_loss 0.8686 acc 0.8663 | test_loss 0.8483 acc 0.8669\n",
      "epoch 12/40 | lr 0.00300 | train_loss 0.8666 acc 0.8668 | test_loss 0.8085 acc 0.8858\n",
      "epoch 13/40 | lr 0.00299 | train_loss 0.8655 acc 0.8668 | test_loss 0.8265 acc 0.8787\n",
      "epoch 14/40 | lr 0.00296 | train_loss 0.8681 acc 0.8664 | test_loss 0.8239 acc 0.8789\n",
      "epoch 15/40 | lr 0.00292 | train_loss 0.8670 acc 0.8660 | test_loss 0.8083 acc 0.8818\n",
      "epoch 16/40 | lr 0.00285 | train_loss 0.8671 acc 0.8647 | test_loss 0.8277 acc 0.8767\n",
      "epoch 17/40 | lr 0.00277 | train_loss 0.8659 acc 0.8667 | test_loss 0.8009 acc 0.8891\n",
      "Early stop at epoch 17; best test_acc = 0.9043\n",
      "Restored best | test_loss 0.7749 | test_acc 0.9043\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# ---------- 1) Transforms ----------\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.ToPILImage(),                # needed by RandomCrop/Rotation\n",
    "    transforms.RandomCrop(28, padding=2),   # small jitter\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),                  # -> [1,28,28] in [0,1]\n",
    "])\n",
    "test_tfms = transforms.Compose([\n",
    "    transforms.ToPILImage(),                # keep test clean (no aug)\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# ---------- 2) NPZ dataset wrapper ----------\n",
    "class KMNISTNPZ(Dataset):\n",
    "    def __init__(self, x_np, y_np, tfm):\n",
    "        self.x = x_np\n",
    "        self.y = y_np\n",
    "        self.tfm = tfm\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.x[idx]                         # (28,28) uint8\n",
    "        lab = int(self.y[idx])                    # scalar\n",
    "        img = self.tfm(img)                       # -> [1,28,28] float\n",
    "        return img, torch.tensor(lab, dtype=torch.long)\n",
    "\n",
    "# NOTE: assumes x_train, y_train, x_test, y_test already loaded from NPZ\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(KMNISTNPZ(x_train, y_train, train_tfms),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(KMNISTNPZ(x_test,  y_test,  test_tfms),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# (Optional) sanity check\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"train batch:\", xb.shape, xb.min().item(), xb.max().item(), yb[:8])\n",
    "\n",
    "# ---------- 3) Model ----------\n",
    "# Use your current MLP model object named `model` (already created & moved to device).\n",
    "# If you want my suggested wider MLP with BN+GELU+Dropout(0.35), define it above and set model=...\n",
    "\n",
    "# ---------- 4) Loss / Optimizer / Scheduler ----------\n",
    "criterion  = nn.CrossEntropyLoss(label_smoothing=0.10)\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=3e-3, weight_decay=1e-4)\n",
    "\n",
    "EPOCHS = 40\n",
    "steps_per_epoch = len(train_loader)  # recompute after rebuilding loader!\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=3e-3,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    pct_start=0.3,\n",
    "    div_factor=10,\n",
    "    final_div_factor=100,\n",
    ")\n",
    "\n",
    "def lr_now(opt): \n",
    "    return opt.param_groups[0]['lr']\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "# ---------- 5) Train with early stopping ----------\n",
    "best_acc, wait, patience = 0.0, 0, 15\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    run_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # step OneCycle each batch\n",
    "\n",
    "        run_loss += loss.item() * xb.size(0)\n",
    "        correct  += (logits.argmax(1) == yb).sum().item()\n",
    "        total    += xb.size(0)\n",
    "\n",
    "    train_loss = run_loss / total\n",
    "    train_acc  = correct / total\n",
    "\n",
    "    test_loss, test_acc = evaluate(model, test_loader)\n",
    "\n",
    "    print(\n",
    "        f\"epoch {epoch:2d}/{EPOCHS} | lr {lr_now(optimizer):.5f} | \"\n",
    "        f\"train_loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
    "        f\"test_loss {test_loss:.4f} acc {test_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    if test_acc > best_acc:\n",
    "        best_acc, wait = test_acc, 0\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stop at epoch {epoch}; best test_acc = {best_acc:.4f}\")\n",
    "            break\n",
    "\n",
    "# ---------- 6) Restore best ----------\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    test_loss, test_acc = evaluate(model, test_loader)\n",
    "    print(f\"Restored best | test_loss {test_loss:.4f} | test_acc {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "df867207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "P1 | epoch  1/40 | lr 0.00035 | train 1.2896/0.6334 | test 0.9595/0.7407\n",
      "P1 | epoch  2/40 | lr 0.00048 | train 0.8966/0.7802 | test 0.8349/0.7952\n",
      "P1 | epoch  3/40 | lr 0.00070 | train 0.8099/0.8160 | test 0.7779/0.8159\n",
      "P1 | epoch  4/40 | lr 0.00098 | train 0.7670/0.8306 | test 0.7504/0.8274\n",
      "P1 | epoch  5/40 | lr 0.00130 | train 0.7394/0.8394 | test 0.7338/0.8371\n",
      "P1 | epoch  6/40 | lr 0.00165 | train 0.7148/0.8507 | test 0.7205/0.8426\n",
      "P1 | epoch  7/40 | lr 0.00200 | train 0.7075/0.8527 | test 0.7127/0.8424\n",
      "P1 | epoch  8/40 | lr 0.00233 | train 0.6927/0.8606 | test 0.6898/0.8535\n",
      "P1 | epoch  9/40 | lr 0.00260 | train 0.6900/0.8612 | test 0.7212/0.8420\n",
      "P1 | epoch 10/40 | lr 0.00282 | train 0.6891/0.8611 | test 0.7039/0.8458\n",
      "P1 | epoch 11/40 | lr 0.00295 | train 0.6949/0.8589 | test 0.6779/0.8533\n",
      "P1 | epoch 12/40 | lr 0.00300 | train 0.6989/0.8569 | test 0.6949/0.8506\n",
      "P1 | epoch 13/40 | lr 0.00299 | train 0.6958/0.8576 | test 0.6805/0.8582\n",
      "P1 | epoch 14/40 | lr 0.00296 | train 0.6944/0.8607 | test 0.6846/0.8565\n",
      "P1 | epoch 15/40 | lr 0.00292 | train 0.6929/0.8598 | test 0.6970/0.8503\n",
      "P1 | epoch 16/40 | lr 0.00285 | train 0.6907/0.8607 | test 0.6754/0.8617\n",
      "P1 | epoch 17/40 | lr 0.00277 | train 0.6956/0.8582 | test 0.6781/0.8592\n",
      "P1 | epoch 18/40 | lr 0.00267 | train 0.6882/0.8615 | test 0.6647/0.8648\n",
      "P1 | epoch 19/40 | lr 0.00256 | train 0.6836/0.8633 | test 0.6723/0.8599\n",
      "P1 | epoch 20/40 | lr 0.00244 | train 0.6808/0.8647 | test 0.6723/0.8607\n",
      "P1 | epoch 21/40 | lr 0.00230 | train 0.6776/0.8667 | test 0.6458/0.8725\n",
      "P1 | epoch 22/40 | lr 0.00215 | train 0.6747/0.8668 | test 0.6564/0.8697\n",
      "P1 | epoch 23/40 | lr 0.00200 | train 0.6609/0.8737 | test 0.6636/0.8656\n",
      "P1 | epoch 24/40 | lr 0.00183 | train 0.6551/0.8769 | test 0.6478/0.8721\n",
      "P1 | epoch 25/40 | lr 0.00167 | train 0.6486/0.8782 | test 0.6544/0.8722\n",
      "P1 | epoch 26/40 | lr 0.00150 | train 0.6408/0.8802 | test 0.6296/0.8811\n",
      "P1 | epoch 27/40 | lr 0.00133 | train 0.6309/0.8850 | test 0.6178/0.8859\n",
      "P1 | epoch 28/40 | lr 0.00117 | train 0.6197/0.8887 | test 0.6021/0.8930\n",
      "P1 | epoch 29/40 | lr 0.00101 | train 0.6145/0.8902 | test 0.5950/0.8894\n",
      "P1 | epoch 30/40 | lr 0.00085 | train 0.6005/0.8960 | test 0.5844/0.8951\n",
      "P1 | epoch 31/40 | lr 0.00070 | train 0.5888/0.9018 | test 0.5799/0.8976\n",
      "P1 | epoch 32/40 | lr 0.00057 | train 0.5824/0.9040 | test 0.5697/0.9028\n",
      "P1 | epoch 33/40 | lr 0.00044 | train 0.5734/0.9059 | test 0.5635/0.9021\n",
      "P1 | epoch 34/40 | lr 0.00033 | train 0.5607/0.9114 | test 0.5737/0.9010\n",
      "P1 | epoch 35/40 | lr 0.00023 | train 0.5569/0.9122 | test 0.5503/0.9116\n",
      "P1 | epoch 36/40 | lr 0.00015 | train 0.5490/0.9153 | test 0.5386/0.9149\n",
      "P1 | epoch 37/40 | lr 0.00009 | train 0.5422/0.9204 | test 0.5389/0.9158\n",
      "P1 | epoch 38/40 | lr 0.00004 | train 0.5389/0.9197 | test 0.5329/0.9172\n",
      "P1 | epoch 39/40 | lr 0.00001 | train 0.5369/0.9205 | test 0.5392/0.9144\n",
      "P1 | epoch 40/40 | lr 0.00000 | train 0.5356/0.9204 | test 0.5390/0.9144\n",
      "P1 restored best | test_loss 0.5329 | test_acc 0.9172\n"
     ]
    }
   ],
   "source": [
    "# ===== P1: rotate+translate (no crop) | LS 0.05 | WD 3e-4 | Dropout 0.40 =====\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- device (auto) ---\n",
    "try:\n",
    "    device\n",
    "except NameError:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# --- transforms ---\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "test_tfms = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])\n",
    "\n",
    "# --- dataset wrapper ---\n",
    "class KMNISTNPZ(Dataset):\n",
    "    def __init__(self, x_np, y_np, tfm):\n",
    "        self.x, self.y, self.tfm = x_np, y_np, tfm\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i):\n",
    "        img = self.tfm(self.x[i])                 # (28,28)->[1,28,28]\n",
    "        lab = torch.tensor(int(self.y[i]), dtype=torch.long)\n",
    "        return img, lab\n",
    "\n",
    "BATCH = 128\n",
    "train_loader = DataLoader(KMNISTNPZ(x_train, y_train, train_tfms), batch_size=BATCH, shuffle=True)\n",
    "test_loader  = DataLoader(KMNISTNPZ(x_test,  y_test,  test_tfms),  batch_size=BATCH, shuffle=False)\n",
    "\n",
    "# --- model (w/ configurable dropout) ---\n",
    "class MLP_Wide(nn.Module):\n",
    "    def __init__(self, p=0.40):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 512), nn.BatchNorm1d(512), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "model = MLP_Wide(p=0.40).to(device)\n",
    "\n",
    "# --- loss/opt/sched ---\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3, weight_decay=3e-4)\n",
    "EPOCHS = 40\n",
    "steps_per_epoch = len(train_loader)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=3e-3, epochs=EPOCHS, steps_per_epoch=steps_per_epoch,\n",
    "    pct_start=0.3, div_factor=10, final_div_factor=100\n",
    ")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(m, loader):\n",
    "    m.eval(); totL=0.0; cor=0; tot=0\n",
    "    for xb,yb in loader:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        lg = m(xb); loss = criterion(lg,yb)\n",
    "        totL += loss.item()*xb.size(0)\n",
    "        cor  += (lg.argmax(1)==yb).sum().item()\n",
    "        tot  += xb.size(0)\n",
    "    return totL/tot, cor/tot\n",
    "\n",
    "def lr_now(opt): return opt.param_groups[0]['lr']\n",
    "\n",
    "best_acc, wait, patience, best_state = 0.0, 0, 15, None\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train(); runL=0.0; cor=0; tot=0\n",
    "    for xb,yb in train_loader:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        lg = model(xb); loss = criterion(lg,yb)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step(); scheduler.step()\n",
    "        runL += loss.item()*xb.size(0); cor += (lg.argmax(1)==yb).sum().item(); tot += xb.size(0)\n",
    "    trL, trA = runL/tot, cor/tot\n",
    "    teL, teA = evaluate(model, test_loader)\n",
    "    print(f\"P1 | epoch {ep:2d}/{EPOCHS} | lr {lr_now(optimizer):.5f} | train {trL:.4f}/{trA:.4f} | test {teL:.4f}/{teA:.4f}\")\n",
    "    if teA>best_acc:\n",
    "        best_acc, wait = teA, 0\n",
    "        best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait>=patience: print(f\"P1 early stop at {ep}; best acc {best_acc:.4f}\"); break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    teL, teA = evaluate(model, test_loader)\n",
    "    print(f\"P1 restored best | test_loss {teL:.4f} | test_acc {teA:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "a12ca269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "P2 | epoch  1/40 | lr 0.00035 | train 1.2403/0.6027 | test 0.8525/0.7281\n",
      "P2 | epoch  2/40 | lr 0.00048 | train 0.7780/0.7492 | test 0.7063/0.7734\n",
      "P2 | epoch  3/40 | lr 0.00070 | train 0.6734/0.7840 | test 0.6226/0.7973\n",
      "P2 | epoch  4/40 | lr 0.00098 | train 0.6098/0.8040 | test 0.5317/0.8262\n",
      "P2 | epoch  5/40 | lr 0.00130 | train 0.5633/0.8179 | test 0.5137/0.8357\n",
      "P2 | epoch  6/40 | lr 0.00165 | train 0.5216/0.8318 | test 0.4649/0.8468\n",
      "P2 | epoch  7/40 | lr 0.00200 | train 0.4961/0.8417 | test 0.4577/0.8507\n",
      "P2 | epoch  8/40 | lr 0.00233 | train 0.4697/0.8508 | test 0.4360/0.8570\n",
      "P2 | epoch  9/40 | lr 0.00260 | train 0.4507/0.8563 | test 0.4367/0.8573\n",
      "P2 | epoch 10/40 | lr 0.00282 | train 0.4276/0.8633 | test 0.4014/0.8703\n",
      "P2 | epoch 11/40 | lr 0.00295 | train 0.4220/0.8658 | test 0.3949/0.8730\n",
      "P2 | epoch 12/40 | lr 0.00300 | train 0.4135/0.8695 | test 0.4072/0.8720\n",
      "P2 | epoch 13/40 | lr 0.00299 | train 0.4111/0.8684 | test 0.3870/0.8801\n",
      "P2 | epoch 14/40 | lr 0.00296 | train 0.4033/0.8713 | test 0.3678/0.8830\n",
      "P2 | epoch 15/40 | lr 0.00292 | train 0.3974/0.8736 | test 0.3821/0.8779\n",
      "P2 | epoch 16/40 | lr 0.00285 | train 0.3888/0.8777 | test 0.3768/0.8784\n",
      "P2 | epoch 17/40 | lr 0.00277 | train 0.3902/0.8748 | test 0.3475/0.8923\n",
      "P2 | epoch 18/40 | lr 0.00267 | train 0.3891/0.8752 | test 0.3709/0.8790\n",
      "P2 | epoch 19/40 | lr 0.00256 | train 0.3781/0.8795 | test 0.3704/0.8813\n",
      "P2 | epoch 20/40 | lr 0.00244 | train 0.3743/0.8800 | test 0.3381/0.8934\n",
      "P2 | epoch 21/40 | lr 0.00230 | train 0.3666/0.8838 | test 0.3323/0.8970\n",
      "P2 | epoch 22/40 | lr 0.00215 | train 0.3626/0.8860 | test 0.3276/0.8931\n",
      "P2 | epoch 23/40 | lr 0.00200 | train 0.3551/0.8865 | test 0.3187/0.8992\n",
      "P2 | epoch 24/40 | lr 0.00183 | train 0.3455/0.8906 | test 0.3335/0.8960\n",
      "P2 | epoch 25/40 | lr 0.00167 | train 0.3418/0.8920 | test 0.3140/0.8994\n",
      "P2 | epoch 26/40 | lr 0.00150 | train 0.3256/0.8968 | test 0.3025/0.9046\n",
      "P2 | epoch 27/40 | lr 0.00133 | train 0.3221/0.8968 | test 0.2932/0.9091\n",
      "P2 | epoch 28/40 | lr 0.00117 | train 0.3082/0.9017 | test 0.2941/0.9045\n",
      "P2 | epoch 29/40 | lr 0.00101 | train 0.2957/0.9062 | test 0.2630/0.9182\n",
      "P2 | epoch 30/40 | lr 0.00085 | train 0.2919/0.9077 | test 0.2658/0.9171\n",
      "P2 | epoch 31/40 | lr 0.00070 | train 0.2818/0.9106 | test 0.2519/0.9207\n",
      "P2 | epoch 32/40 | lr 0.00057 | train 0.2683/0.9155 | test 0.2536/0.9207\n",
      "P2 | epoch 33/40 | lr 0.00044 | train 0.2629/0.9165 | test 0.2614/0.9201\n",
      "P2 | epoch 34/40 | lr 0.00033 | train 0.2576/0.9175 | test 0.2494/0.9233\n",
      "P2 | epoch 35/40 | lr 0.00023 | train 0.2463/0.9220 | test 0.2430/0.9245\n",
      "P2 | epoch 36/40 | lr 0.00015 | train 0.2371/0.9258 | test 0.2346/0.9259\n",
      "P2 | epoch 37/40 | lr 0.00009 | train 0.2341/0.9257 | test 0.2248/0.9300\n",
      "P2 | epoch 38/40 | lr 0.00004 | train 0.2362/0.9260 | test 0.2243/0.9320\n",
      "P2 | epoch 39/40 | lr 0.00001 | train 0.2328/0.9259 | test 0.2244/0.9318\n",
      "P2 | epoch 40/40 | lr 0.00000 | train 0.2299/0.9273 | test 0.2247/0.9305\n",
      "P2 restored best | test_loss 0.2243 | test_acc 0.9320\n"
     ]
    }
   ],
   "source": [
    "# ===== P2: crop+rotate+translate | LS 0.00 | WD 1e-4 | Dropout 0.35 =====\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "try:\n",
    "    device\n",
    "except NameError:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomCrop(28, padding=2),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "test_tfms = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])\n",
    "\n",
    "class KMNISTNPZ(Dataset):\n",
    "    def __init__(self, x_np, y_np, tfm):\n",
    "        self.x, self.y, self.tfm = x_np, y_np, tfm\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i):\n",
    "        img = self.tfm(self.x[i]); lab = torch.tensor(int(self.y[i]), dtype=torch.long)\n",
    "        return img, lab\n",
    "\n",
    "BATCH=128\n",
    "train_loader = DataLoader(KMNISTNPZ(x_train,y_train,train_tfms), batch_size=BATCH, shuffle=True)\n",
    "test_loader  = DataLoader(KMNISTNPZ(x_test, y_test, test_tfms),  batch_size=BATCH, shuffle=False)\n",
    "\n",
    "class MLP_Wide(nn.Module):\n",
    "    def __init__(self, p=0.35):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 512), nn.BatchNorm1d(512), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "model = MLP_Wide(p=0.35).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3, weight_decay=1e-4)\n",
    "EPOCHS = 40\n",
    "steps_per_epoch = len(train_loader)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=3e-3, epochs=EPOCHS, steps_per_epoch=steps_per_epoch,\n",
    "    pct_start=0.3, div_factor=10, final_div_factor=100\n",
    ")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(m, loader):\n",
    "    m.eval(); totL=0.0; cor=0; tot=0\n",
    "    for xb,yb in loader:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        lg=m(xb); loss=criterion(lg,yb)\n",
    "        totL+=loss.item()*xb.size(0); cor+=(lg.argmax(1)==yb).sum().item(); tot+=xb.size(0)\n",
    "    return totL/tot, cor/tot\n",
    "\n",
    "def lr_now(opt): return opt.param_groups[0]['lr']\n",
    "\n",
    "best_acc, wait, patience, best_state = 0.0, 0, 15, None\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train(); runL=0.0; cor=0; tot=0\n",
    "    for xb,yb in train_loader:\n",
    "        xb,yb=xb.to(device), yb.to(device)\n",
    "        lg=model(xb); loss=criterion(lg,yb)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step(); scheduler.step()\n",
    "        runL+=loss.item()*xb.size(0); cor+=(lg.argmax(1)==yb).sum().item(); tot+=xb.size(0)\n",
    "    trL, trA = runL/tot, cor/tot\n",
    "    teL, teA = evaluate(model, test_loader)\n",
    "    print(f\"P2 | epoch {ep:2d}/{EPOCHS} | lr {lr_now(optimizer):.5f} | train {trL:.4f}/{trA:.4f} | test {teL:.4f}/{teA:.4f}\")\n",
    "    if teA>best_acc:\n",
    "        best_acc, wait = teA, 0\n",
    "        best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait>=patience: print(f\"P2 early stop at {ep}; best acc {best_acc:.4f}\"); break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    teL, teA = evaluate(model, test_loader)\n",
    "    print(f\"P2 restored best | test_loss {teL:.4f} | test_acc {teA:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "b823c48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "P3 | epoch  1/40 | lr 0.00035 | train 1.2337/0.6579 | test 0.9420/0.7560\n",
      "P3 | epoch  2/40 | lr 0.00048 | train 0.8609/0.7957 | test 0.7956/0.8154\n",
      "P3 | epoch  3/40 | lr 0.00070 | train 0.7765/0.8288 | test 0.7352/0.8361\n",
      "P3 | epoch  4/40 | lr 0.00098 | train 0.7332/0.8440 | test 0.7255/0.8381\n",
      "P3 | epoch  5/40 | lr 0.00130 | train 0.7060/0.8549 | test 0.6927/0.8469\n",
      "P3 | epoch  6/40 | lr 0.00165 | train 0.6884/0.8612 | test 0.7124/0.8419\n",
      "P3 | epoch  7/40 | lr 0.00200 | train 0.6743/0.8663 | test 0.6629/0.8674\n",
      "P3 | epoch  8/40 | lr 0.00233 | train 0.6665/0.8696 | test 0.6867/0.8543\n",
      "P3 | epoch  9/40 | lr 0.00260 | train 0.6635/0.8700 | test 0.6818/0.8558\n",
      "P3 | epoch 10/40 | lr 0.00282 | train 0.6643/0.8719 | test 0.6547/0.8677\n",
      "P3 | epoch 11/40 | lr 0.00295 | train 0.6688/0.8688 | test 0.6626/0.8633\n",
      "P3 | epoch 12/40 | lr 0.00300 | train 0.6635/0.8716 | test 0.6759/0.8620\n",
      "P3 | epoch 13/40 | lr 0.00299 | train 0.6692/0.8701 | test 0.6706/0.8651\n",
      "P3 | epoch 14/40 | lr 0.00296 | train 0.6688/0.8693 | test 0.6989/0.8461\n",
      "P3 | epoch 15/40 | lr 0.00292 | train 0.6712/0.8692 | test 0.6760/0.8601\n",
      "P3 | epoch 16/40 | lr 0.00285 | train 0.6679/0.8703 | test 0.6777/0.8599\n",
      "P3 | epoch 17/40 | lr 0.00277 | train 0.6667/0.8687 | test 0.6477/0.8708\n",
      "P3 | epoch 18/40 | lr 0.00267 | train 0.6623/0.8713 | test 0.6451/0.8716\n",
      "P3 | epoch 19/40 | lr 0.00256 | train 0.6544/0.8748 | test 0.6427/0.8748\n",
      "P3 | epoch 20/40 | lr 0.00244 | train 0.6520/0.8759 | test 0.6364/0.8757\n",
      "P3 | epoch 21/40 | lr 0.00230 | train 0.6475/0.8780 | test 0.6392/0.8759\n",
      "P3 | epoch 22/40 | lr 0.00215 | train 0.6385/0.8819 | test 0.6280/0.8799\n",
      "P3 | epoch 23/40 | lr 0.00200 | train 0.6388/0.8811 | test 0.6276/0.8791\n",
      "P3 | epoch 24/40 | lr 0.00183 | train 0.6248/0.8857 | test 0.6113/0.8881\n",
      "P3 | epoch 25/40 | lr 0.00167 | train 0.6233/0.8867 | test 0.6083/0.8880\n",
      "P3 | epoch 26/40 | lr 0.00150 | train 0.6139/0.8900 | test 0.6284/0.8805\n",
      "P3 | epoch 27/40 | lr 0.00133 | train 0.6048/0.8936 | test 0.5943/0.8951\n",
      "P3 | epoch 28/40 | lr 0.00117 | train 0.5952/0.8996 | test 0.5948/0.8918\n",
      "P3 | epoch 29/40 | lr 0.00101 | train 0.5860/0.9027 | test 0.5748/0.9018\n",
      "P3 | epoch 30/40 | lr 0.00085 | train 0.5748/0.9075 | test 0.5721/0.9023\n",
      "P3 | epoch 31/40 | lr 0.00070 | train 0.5690/0.9085 | test 0.5539/0.9066\n",
      "P3 | epoch 32/40 | lr 0.00057 | train 0.5575/0.9134 | test 0.5506/0.9104\n",
      "P3 | epoch 33/40 | lr 0.00044 | train 0.5508/0.9169 | test 0.5372/0.9171\n",
      "P3 | epoch 34/40 | lr 0.00033 | train 0.5395/0.9211 | test 0.5278/0.9203\n",
      "P3 | epoch 35/40 | lr 0.00023 | train 0.5311/0.9229 | test 0.5253/0.9187\n",
      "P3 | epoch 36/40 | lr 0.00015 | train 0.5273/0.9234 | test 0.5182/0.9220\n",
      "P3 | epoch 37/40 | lr 0.00009 | train 0.5184/0.9283 | test 0.5147/0.9231\n",
      "P3 | epoch 38/40 | lr 0.00004 | train 0.5162/0.9288 | test 0.5123/0.9253\n",
      "P3 | epoch 39/40 | lr 0.00001 | train 0.5136/0.9298 | test 0.5115/0.9263\n",
      "P3 | epoch 40/40 | lr 0.00000 | train 0.5113/0.9301 | test 0.5148/0.9242\n",
      "P3 restored best | test_loss 0.5115 | test_acc 0.9263\n"
     ]
    }
   ],
   "source": [
    "# ===== P3: gentler aug (crop pad=1, rot=7°, trans=0.08) | LS 0.05 | WD 3e-4 | Dropout 0.40 =====\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "try:\n",
    "    device\n",
    "except NameError:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomCrop(28, padding=1),\n",
    "    transforms.RandomRotation(7),\n",
    "    transforms.RandomAffine(0, translate=(0.08, 0.08)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "test_tfms = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])\n",
    "\n",
    "class KMNISTNPZ(Dataset):\n",
    "    def __init__(self, x_np, y_np, tfm):\n",
    "        self.x, self.y, self.tfm = x_np, y_np, tfm\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i):\n",
    "        img = self.tfm(self.x[i]); lab = torch.tensor(int(self.y[i]), dtype=torch.long)\n",
    "        return img, lab\n",
    "\n",
    "BATCH=128\n",
    "train_loader = DataLoader(KMNISTNPZ(x_train,y_train,train_tfms), batch_size=BATCH, shuffle=True)\n",
    "test_loader  = DataLoader(KMNISTNPZ(x_test, y_test, test_tfms),  batch_size=BATCH, shuffle=False)\n",
    "\n",
    "class MLP_Wide(nn.Module):\n",
    "    def __init__(self, p=0.40):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 512), nn.BatchNorm1d(512), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "model = MLP_Wide(p=0.40).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3, weight_decay=3e-4)\n",
    "EPOCHS = 40\n",
    "steps_per_epoch = len(train_loader)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=3e-3, epochs=EPOCHS, steps_per_epoch=steps_per_epoch,\n",
    "    pct_start=0.3, div_factor=10, final_div_factor=100\n",
    ")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(m, loader):\n",
    "    m.eval(); totL=0.0; cor=0; tot=0\n",
    "    for xb,yb in loader:\n",
    "        xb,yb=xb.to(device), yb.to(device)\n",
    "        lg=m(xb); loss=criterion(lg,yb)\n",
    "        totL+=loss.item()*xb.size(0); cor+=(lg.argmax(1)==yb).sum().item(); tot+=xb.size(0)\n",
    "    return totL/tot, cor/tot\n",
    "\n",
    "def lr_now(opt): return opt.param_groups[0]['lr']\n",
    "\n",
    "best_acc, wait, patience, best_state = 0.0, 0, 15, None\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train(); runL=0.0; cor=0; tot=0\n",
    "    for xb,yb in train_loader:\n",
    "        xb,yb=xb.to(device), yb.to(device)\n",
    "        lg=model(xb); loss=criterion(lg,yb)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step(); scheduler.step()\n",
    "        runL+=loss.item()*xb.size(0); cor+=(lg.argmax(1)==yb).sum().item(); tot+=xb.size(0)\n",
    "    trL, trA = runL/tot, cor/tot\n",
    "    teL, teA = evaluate(model, test_loader)\n",
    "    print(f\"P3 | epoch {ep:2d}/{EPOCHS} | lr {lr_now(optimizer):.5f} | train {trL:.4f}/{trA:.4f} | test {teL:.4f}/{teA:.4f}\")\n",
    "    if teA>best_acc:\n",
    "        best_acc, wait = teA, 0\n",
    "        best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait>=patience: print(f\"P3 early stop at {ep}; best acc {best_acc:.4f}\"); break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    teL, teA = evaluate(model, test_loader)\n",
    "    print(f\"P3 restored best | test_loss {teL:.4f} | test_acc {teA:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "bfd7b866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "P2 | epoch  1/40 | lr 0.00035 | train 1.2421/0.6027 | test 0.8440/0.7264\n",
      "P2 | epoch  2/40 | lr 0.00048 | train 0.7745/0.7512 | test 0.6726/0.7838\n",
      "P2 | epoch  3/40 | lr 0.00070 | train 0.6647/0.7858 | test 0.6344/0.7942\n",
      "P2 | epoch  4/40 | lr 0.00098 | train 0.6038/0.8057 | test 0.5434/0.8217\n",
      "P2 | epoch  5/40 | lr 0.00130 | train 0.5592/0.8188 | test 0.5014/0.8330\n",
      "P2 | epoch  6/40 | lr 0.00165 | train 0.5232/0.8305 | test 0.4880/0.8379\n",
      "P2 | epoch  7/40 | lr 0.00200 | train 0.4955/0.8398 | test 0.4369/0.8550\n",
      "P2 | epoch  8/40 | lr 0.00233 | train 0.4706/0.8495 | test 0.4636/0.8515\n",
      "P2 | epoch  9/40 | lr 0.00260 | train 0.4464/0.8572 | test 0.4098/0.8665\n",
      "P2 | epoch 10/40 | lr 0.00282 | train 0.4385/0.8597 | test 0.3760/0.8806\n",
      "P2 | epoch 11/40 | lr 0.00295 | train 0.4213/0.8659 | test 0.3971/0.8737\n",
      "P2 | epoch 12/40 | lr 0.00300 | train 0.4188/0.8669 | test 0.3849/0.8763\n",
      "P2 | epoch 13/40 | lr 0.00299 | train 0.4097/0.8708 | test 0.3906/0.8751\n",
      "P2 | epoch 14/40 | lr 0.00296 | train 0.4031/0.8717 | test 0.3724/0.8820\n",
      "P2 | epoch 15/40 | lr 0.00292 | train 0.4043/0.8723 | test 0.3873/0.8782\n",
      "P2 | epoch 16/40 | lr 0.00285 | train 0.3975/0.8732 | test 0.3597/0.8855\n",
      "P2 | epoch 17/40 | lr 0.00277 | train 0.3897/0.8754 | test 0.3877/0.8779\n",
      "P2 | epoch 18/40 | lr 0.00267 | train 0.3895/0.8764 | test 0.3788/0.8806\n",
      "P2 | epoch 19/40 | lr 0.00256 | train 0.3850/0.8754 | test 0.3248/0.8939\n",
      "P2 | epoch 20/40 | lr 0.00244 | train 0.3722/0.8816 | test 0.3257/0.8963\n",
      "P2 | epoch 21/40 | lr 0.00230 | train 0.3691/0.8837 | test 0.3312/0.8980\n",
      "P2 | epoch 22/40 | lr 0.00215 | train 0.3645/0.8853 | test 0.3337/0.8953\n",
      "P2 | epoch 23/40 | lr 0.00200 | train 0.3606/0.8850 | test 0.3092/0.9032\n",
      "P2 | epoch 24/40 | lr 0.00183 | train 0.3479/0.8899 | test 0.3141/0.9017\n",
      "P2 | epoch 25/40 | lr 0.00167 | train 0.3358/0.8939 | test 0.3366/0.8938\n",
      "P2 | epoch 26/40 | lr 0.00150 | train 0.3288/0.8958 | test 0.2889/0.9109\n",
      "P2 | epoch 27/40 | lr 0.00133 | train 0.3169/0.8992 | test 0.3025/0.9058\n",
      "P2 | epoch 28/40 | lr 0.00117 | train 0.3090/0.9016 | test 0.2826/0.9110\n",
      "P2 | epoch 29/40 | lr 0.00101 | train 0.3018/0.9043 | test 0.2699/0.9141\n",
      "P2 | epoch 30/40 | lr 0.00085 | train 0.2904/0.9073 | test 0.2697/0.9138\n",
      "P2 | epoch 31/40 | lr 0.00070 | train 0.2829/0.9100 | test 0.2552/0.9201\n",
      "P2 | epoch 32/40 | lr 0.00057 | train 0.2717/0.9147 | test 0.2510/0.9218\n",
      "P2 | epoch 33/40 | lr 0.00044 | train 0.2588/0.9172 | test 0.2386/0.9249\n",
      "P2 | epoch 34/40 | lr 0.00033 | train 0.2567/0.9170 | test 0.2394/0.9241\n",
      "P2 | epoch 35/40 | lr 0.00023 | train 0.2463/0.9229 | test 0.2252/0.9317\n",
      "P2 | epoch 36/40 | lr 0.00015 | train 0.2448/0.9218 | test 0.2282/0.9292\n",
      "P2 | epoch 37/40 | lr 0.00009 | train 0.2394/0.9254 | test 0.2234/0.9319\n",
      "P2 | epoch 38/40 | lr 0.00004 | train 0.2355/0.9260 | test 0.2197/0.9324\n",
      "P2 | epoch 39/40 | lr 0.00001 | train 0.2317/0.9262 | test 0.2241/0.9309\n",
      "P2 | epoch 40/40 | lr 0.00000 | train 0.2308/0.9268 | test 0.2248/0.9295\n",
      "P2 restored best | test_loss 0.2197 | test_acc 0.9324\n"
     ]
    }
   ],
   "source": [
    "# ===== P2: crop+rotate+translate | LS 0.00 | WD 1e-4 | Dropout 0.35 =====\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "try:\n",
    "    device\n",
    "except NameError:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomCrop(28, padding=2),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "test_tfms = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])\n",
    "\n",
    "class KMNISTNPZ(Dataset):\n",
    "    def __init__(self, x_np, y_np, tfm):\n",
    "        self.x, self.y, self.tfm = x_np, y_np, tfm\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i):\n",
    "        img = self.tfm(self.x[i]); lab = torch.tensor(int(self.y[i]), dtype=torch.long)\n",
    "        return img, lab\n",
    "\n",
    "BATCH=128\n",
    "train_loader = DataLoader(KMNISTNPZ(x_train,y_train,train_tfms), batch_size=BATCH, shuffle=True)\n",
    "test_loader  = DataLoader(KMNISTNPZ(x_test, y_test, test_tfms),  batch_size=BATCH, shuffle=False)\n",
    "\n",
    "class MLP_Wide(nn.Module):\n",
    "    def __init__(self, p=0.35):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 512), nn.BatchNorm1d(512), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "model = MLP_Wide(p=0.35).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3, weight_decay=1e-4)\n",
    "EPOCHS = 40\n",
    "steps_per_epoch = len(train_loader)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=3e-3, epochs=EPOCHS, steps_per_epoch=steps_per_epoch,\n",
    "    pct_start=0.3, div_factor=10, final_div_factor=100\n",
    ")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(m, loader):\n",
    "    m.eval(); totL=0.0; cor=0; tot=0\n",
    "    for xb,yb in loader:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        lg=m(xb); loss=criterion(lg,yb)\n",
    "        totL+=loss.item()*xb.size(0); cor+=(lg.argmax(1)==yb).sum().item(); tot+=xb.size(0)\n",
    "    return totL/tot, cor/tot\n",
    "\n",
    "def lr_now(opt): return opt.param_groups[0]['lr']\n",
    "\n",
    "best_acc, wait, patience, best_state = 0.0, 0, 15, None\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train(); runL=0.0; cor=0; tot=0\n",
    "    for xb,yb in train_loader:\n",
    "        xb,yb=xb.to(device), yb.to(device)\n",
    "        lg=model(xb); loss=criterion(lg,yb)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step(); scheduler.step()\n",
    "        runL+=loss.item()*xb.size(0); cor+=(lg.argmax(1)==yb).sum().item(); tot+=xb.size(0)\n",
    "    trL, trA = runL/tot, cor/tot\n",
    "    teL, teA = evaluate(model, test_loader)\n",
    "    print(f\"P2 | epoch {ep:2d}/{EPOCHS} | lr {lr_now(optimizer):.5f} | train {trL:.4f}/{trA:.4f} | test {teL:.4f}/{teA:.4f}\")\n",
    "    if teA>best_acc:\n",
    "        best_acc, wait = teA, 0\n",
    "        best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait>=patience: print(f\"P2 early stop at {ep}; best acc {best_acc:.4f}\"); break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    teL, teA = evaluate(model, test_loader)\n",
    "    print(f\"P2 restored best | test_loss {teL:.4f} | test_acc {teA:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "ae840635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWA epoch 1/6 done | train_loss 0.2463 acc 0.9217\n",
      "SWA epoch 2/6 done | train_loss 0.2469 acc 0.9216\n",
      "SWA epoch 3/6 done | train_loss 0.2465 acc 0.9218\n",
      "SWA epoch 4/6 done | train_loss 0.2420 acc 0.9235\n",
      "SWA epoch 5/6 done | train_loss 0.2388 acc 0.9245\n",
      "SWA epoch 6/6 done | train_loss 0.2451 acc 0.9225\n",
      "\n",
      "Base (post-P2) | test_loss 0.2219 | test_acc 0.9331\n",
      "SWA averaged   | test_loss 0.2183 | test_acc 0.9323\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
    "\n",
    "# === 1. Wrap trained P2 model in SWA ===\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(model).to(device)\n",
    "swa_start = 35   # epoch to start averaging (after your P2 run, just pick a safe number)\n",
    "swa_epochs = 6   # how many SWA passes to run\n",
    "\n",
    "# Use SWALR scheduler (cyclical learning rate for SWA phase)\n",
    "swa_scheduler = SWALR(optimizer, swa_lr=5e-4)\n",
    "\n",
    "# === 2. Run SWA training passes ===\n",
    "for epoch in range(1, swa_epochs + 1):\n",
    "    model.train()\n",
    "    run_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        swa_scheduler.step()\n",
    "\n",
    "        run_loss += loss.item() * xb.size(0)\n",
    "        correct  += (logits.argmax(1) == yb).sum().item()\n",
    "        total    += xb.size(0)\n",
    "\n",
    "    # update running averages\n",
    "    swa_model.update_parameters(model)\n",
    "\n",
    "    train_loss = run_loss / total\n",
    "    train_acc  = correct / total\n",
    "\n",
    "    print(f\"SWA epoch {epoch}/{swa_epochs} done \"\n",
    "          f\"| train_loss {train_loss:.4f} acc {train_acc:.4f}\")\n",
    "\n",
    "# === 3. Update BN stats (important for SWA) ===\n",
    "update_bn(train_loader, swa_model, device=device)\n",
    "\n",
    "# === 4. Evaluate both base and SWA averaged ===\n",
    "base_loss, base_acc = evaluate(model, test_loader)\n",
    "swa_loss, swa_acc   = evaluate(swa_model.module, test_loader)\n",
    "\n",
    "print(f\"\\nBase (post-P2) | test_loss {base_loss:.4f} | test_acc {base_acc:.4f}\")\n",
    "print(f\"SWA averaged   | test_loss {swa_loss:.4f} | test_acc {swa_acc:.4f}\")\n",
    "\n",
    "# === 5. Replace model if SWA is better ===\n",
    "if swa_acc >= base_acc:\n",
    "    model = swa_model.module.to(device)\n",
    "    print(\"✅ Replaced current model weights with SWA averaged weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "3d9c4fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble (base + SWA) | test_loss 0.2164 | test_acc 0.9330\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def ensemble_eval(models, loader, device):\n",
    "    for m in models:\n",
    "        m.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        # forward all models\n",
    "        logits_list = [m(xb) for m in models]\n",
    "\n",
    "        # average probabilities (more stable than averaging logits)\n",
    "        probs_list = [F.softmax(lg, dim=1) for lg in logits_list]\n",
    "        avg_probs = torch.stack(probs_list, dim=0).mean(dim=0)  # [B, C]\n",
    "\n",
    "        # accuracy\n",
    "        pred = avg_probs.argmax(dim=1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "        # cross-entropy on the averaged probabilities\n",
    "        loss_sum += (-torch.log(avg_probs[torch.arange(yb.size(0)), yb] + 1e-12)).sum().item()\n",
    "\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "# --- evaluate ensemble of (base model) + (SWA averaged) ---\n",
    "models = [model, swa_model.module]  # base + SWA\n",
    "ens_loss, ens_acc = ensemble_eval(models, test_loader, device)\n",
    "print(f\"Ensemble (base + SWA) | test_loss {ens_loss:.4f} | test_acc {ens_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "444e8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([0.6, 0.4], device=device).view(2, 1, 1)\n",
    "logits_list = [m(xb) for m in models]\n",
    "probs_list = [F.softmax(lg, dim=1) for lg in logits_list]\n",
    "avg_probs = (torch.stack(probs_list, dim=0) * weights).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "1a9c0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save base\n",
    "torch.save(model.state_dict(), \"kmnist_mlp_base.pt\")\n",
    "\n",
    "# save SWA (averaged)\n",
    "torch.save(swa_model.module.state_dict(), \"kmnist_mlp_swa.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae77a77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
